{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = []\n",
    "with open('ATIS_train.iob','r') as f:\n",
    "    for line in f:\n",
    "        raw_train.append(line)\n",
    "        \n",
    "raw_test = []\n",
    "with open('ATIS_test.iob','r') as f:\n",
    "    for line in f:\n",
    "        raw_test.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw=raw_train+raw_test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4978,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(raw_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(893,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(raw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab=[a.split() for a in raw_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd=[a.split() for a in raw_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BOS'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cd[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=[]\n",
    "\n",
    "for sentence in ab:\n",
    "    a=len(sentence)\n",
    "    train_label.append(sentence[a-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label=[]\n",
    "\n",
    "for sentence in cd:\n",
    "    a=len(sentence)\n",
    "    test_label.append(sentence[a-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'atis_city'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label[3879]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR_REMOVING_MULTIPLE_LABEL_AND_CHOOSING_THE_FIRST_ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570\n",
      "602\n",
      "719\n",
      "859\n",
      "1325\n",
      "1769\n",
      "2019\n",
      "2030\n",
      "2342\n",
      "2693\n",
      "2811\n",
      "2874\n",
      "2954\n",
      "3155\n",
      "3266\n",
      "3336\n",
      "3654\n",
      "3746\n",
      "3878\n",
      "3971\n",
      "4207\n",
      "4373\n",
      "4443\n",
      "4612\n",
      "4814\n",
      "4923\n",
      "26\n",
      "4978\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "aa=-1\n",
    "train2=list()\n",
    "for word in train_label:\n",
    "    aa=aa+1\n",
    "    if '#' in word:\n",
    "        print aa\n",
    "        #print word\n",
    "        for j in range(len(word)):\n",
    "            if word[j]=='#':\n",
    "                train_label[aa]=train_label[aa].replace(train_label[aa],word[0:j])\n",
    "        i=i+1\n",
    "print i\n",
    "print len(train_label)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "206\n",
      "207\n",
      "208\n",
      "213\n",
      "229\n",
      "405\n",
      "406\n",
      "407\n",
      "492\n",
      "497\n",
      "498\n",
      "499\n",
      "604\n",
      "642\n",
      "893\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "aa=-1\n",
    "train2=list()\n",
    "for word in test_label:\n",
    "    aa=aa+1\n",
    "    if '#' in word:\n",
    "        print aa\n",
    "        #print word\n",
    "        for j in range(len(word)):\n",
    "            if word[j]=='#':\n",
    "                test_label[aa]=test_label[aa].replace(test_label[aa],word[0:j])\n",
    "        i=i+1\n",
    "print len(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/sinchan/Videos/slot_filling/train_label.pickle', 'w+') as fp:\n",
    "     pickle.dump(train_label,fp)\n",
    "fp.close()\n",
    "with open('/home/sinchan/Videos/slot_filling/test_label.pickle', 'w+') as fp:\n",
    "     pickle.dump(test_label,fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(893,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4978,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=train_label+test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict=dict()\n",
    "a=0\n",
    "for word in label:\n",
    "    if(word not in label_dict):\n",
    "            label_dict[word] = a\n",
    "            a = a + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/sinchan/Videos/slot_filling/label_dict.pickle', 'w+') as fp:\n",
    "     pickle.dump(label_dict,fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(label_dict)\n",
    "\n",
    "y_train_label = list()\n",
    "for intent in train_label:\n",
    "    out = np.zeros(n)\n",
    "    out = list(out)\n",
    "    out[label_dict[intent]] = 1\n",
    "    y_train_label.append(out)\n",
    "y_test_label = list()\n",
    "for intent in test_label:\n",
    "    out = np.zeros(n)\n",
    "    out = list(out)\n",
    "    out[label_dict[intent]] = 1\n",
    "    y_test_label.append(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/sinchan/Videos/slot_filling/y_test_label.pickle', 'w+') as fp:\n",
    "     pickle.dump(y_test_label,fp)\n",
    "fp.close()\n",
    "with open('/home/sinchan/Videos/slot_filling/y_train_label.pickle', 'w+') as fp:\n",
    "     pickle.dump(y_train_label,fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'atis_abbreviation': 8,\n",
       " 'atis_aircraft': 3,\n",
       " 'atis_aircraft#atis_flight': 16,\n",
       " 'atis_airfare': 2,\n",
       " 'atis_airline': 6,\n",
       " 'atis_airport': 5,\n",
       " 'atis_capacity': 13,\n",
       " 'atis_city': 11,\n",
       " 'atis_distance': 7,\n",
       " 'atis_flight': 0,\n",
       " 'atis_flight_no': 12,\n",
       " 'atis_flight_time': 1,\n",
       " 'atis_ground_fare': 9,\n",
       " 'atis_ground_service': 4,\n",
       " 'atis_meal': 14,\n",
       " 'atis_quantity': 10,\n",
       " 'atis_restriction': 15}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "replace_dic = {'\\'d' : \"would\" , '\\'s' : \"is\" , '\\'t' : \"not\"}\n",
    "for sentence in raw_train:\n",
    "    sentence = sentence.split('\\t')[0][4:-4]\n",
    "    words = sentence.split(' ')\n",
    "    new_sentence = \"\"\n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        word = word.split(\"'\")[0]\n",
    "        if(word == ''):\n",
    "            print word\n",
    "            continue;\n",
    "        if(word in replace_dic):\n",
    "            print(word)\n",
    "            word = replace_dic[word];\n",
    "        if(word.isdigit()):\n",
    "            word = \"number\"\n",
    "        new_sentence = new_sentence + \" \" + word\n",
    "    train.append(new_sentence[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "replace_dic = {'\\'d' : \"would\" , '\\'s' : \"is\" , '\\'t' : \"not\"}\n",
    "for sentence in raw_test:\n",
    "    sentence = sentence.split('\\t')[0][4:-4]\n",
    "    words = sentence.split(' ')\n",
    "    new_sentence = \"\"\n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        word = word.split(\"'\")[0]\n",
    "        if(word == ''):\n",
    "            continue;\n",
    "        if(word in replace_dic):\n",
    "            word = replace_dic[word];\n",
    "        if(word.isdigit()):\n",
    "            word = \"number\"\n",
    "        new_sentence = new_sentence + \" \" + word\n",
    "    test.append(new_sentence[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/sinchan/Videos/slot_filling/train_data.pickle', 'w+') as fp:\n",
    "     pickle.dump(train,fp)\n",
    "fp.close()\n",
    "\n",
    "with open('/home/sinchan/Videos/slot_filling/test_data.pickle', 'w+') as fp:\n",
    "     pickle.dump(test,fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/sinchan/Videos/slot_filling/train_data.pickle', 'r+') as fp:\n",
    "    train = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "with open('/home/sinchan/Videos/slot_filling/test_data.pickle', 'r+') as fp:\n",
    "    test = pickle.load(fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i would like to find a flight from charlotte to las vegas that makes a stop in st. louis'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BOS i would like to find a flight from charlotte to las vegas that makes a stop in st. louis EOS\\tO O O O O O O O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name O O O O O B-stoploc.city_name I-stoploc.city_name atis_flight\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = raw_train + raw_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(word):\n",
    "    replace_dic = {'\\'d' : \"would\" , '\\'s' : \"is\" , '\\'t' : \"not\"}\n",
    "    word = word.split(\"'\")[0]\n",
    "    if(word == ''):\n",
    "        return word\n",
    "    if(word in replace_dic):\n",
    "        word = replace_dic[word];\n",
    "    if(word.isdigit()):\n",
    "        word = \"number\"\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot_dict = dict()\n",
    "word_dict = dict()\n",
    "\n",
    "i_s = 0\n",
    "i_w = 1\n",
    "\n",
    "for utterance in raw:\n",
    "    t = utterance.split('\\t')\n",
    "    words = t[0].strip().split(' ')\n",
    "    slots = t[1].strip().split(' ')\n",
    "    n = len(words)\n",
    "    for i in xrange(n):\n",
    "        word = check(words[i])\n",
    "        if(word not in word_dict):\n",
    "            word_dict[word] = i_w\n",
    "            i_w = i_w + 1\n",
    "    for i in xrange(n-1):\n",
    "        if(slots[i] not in slot_dict):\n",
    "            slot_dict[slots[i]] = i_s\n",
    "            i_s = i_s + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(slot_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = [slot_dict,word_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/sinchan/Videos/slot_filling/slot_word_dict.pickle', 'w+') as fp:\n",
    "    pickle.dump(dicts,fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/sinchan/Videos/slot_filling/slot_word_dict.pickle', 'r+') as fp:\n",
    "    dicts = pickle.load(fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot_dict,word_dict = dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "766"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences1(data,word_dict):\n",
    "    sequences = list()\n",
    "    for sentence in data:\n",
    "        sequence = list()\n",
    "        for word in sentence:\n",
    "                sequence.append(word_dict[word])\n",
    "        sequences.append(sequence)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences(data,word_dict):\n",
    "    sequences = list()\n",
    "    for sentence in data:\n",
    "        sequence = list()\n",
    "        words = sentence.split(' ')\n",
    "        for word in words:\n",
    "                sequence.append(word_dict[word])\n",
    "        sequences.append(sequence)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = make_sequences(train,word_dict)\n",
    "test_sequence = make_sequences(test,word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BOS i want to fly from boston at 838 am and arrive in denver at 1110 in the morning EOS\\t O O O O O O B-fromloc.city_name O B-depart_time.time I-depart_time.time O O O B-toloc.city_name O B-arrive_time.time O O B-arrive_time.period_of_day atis_flight\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_sequence = make_sequences(train,word_dict)\n",
    "test_sequence = make_sequences(test,word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 81,\n",
       " 82,\n",
       " 4,\n",
       " 111,\n",
       " 46,\n",
       " 32,\n",
       " 6,\n",
       " 144,\n",
       " 4,\n",
       " 260,\n",
       " 261,\n",
       " 143,\n",
       " 345,\n",
       " 46,\n",
       " 223,\n",
       " 13,\n",
       " 115,\n",
       " 116]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/sinchan/Videos/slot_filling/train_sequence.pickle', 'w+') as fp:\n",
    "     pickle.dump(train_sequence,fp)\n",
    "fp.close()\n",
    "\n",
    "with open('/home/sinchan/Videos/slot_filling/test_sequence.pickle', 'w+') as fp:\n",
    "     pickle.dump(test_sequence,fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/sinchan/Videos/slot_filling/train_sequence.pickle', 'r+') as fp:\n",
    "    train_sequence = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "with open('/home/sinchan/Videos/slot_filling/test_sequence.pickle', 'r+') as fp:\n",
    "    test_sequence = pickle.load(fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = train_sequence + test_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "m = 0\n",
    "lens = list()\n",
    "for sequence in sequences:\n",
    "    lens.append(len(sequence))\n",
    "    m = max(len(sequence),m)\n",
    "    \n",
    "print m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0b150d11d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.hist(lens,46)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## One-Hot (Slot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BOS find me a flight that flies from memphis to tacoma EOS',\n",
       " 'O O O O O O O O B-fromloc.city_name O B-toloc.city_name atis_flight\\n']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterance = raw_train[0]\n",
    "slots = utterance.split('\\t')[1].strip().split(' ')[:-1]\n",
    "slots = t\n",
    "slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(slot_dict)\n",
    "\n",
    "y_train_slot = list()\n",
    "for utterance in raw_train:\n",
    "    outs = list()\n",
    "    slots = utterance.split('\\t')[1].strip().split(' ')[:-1]\n",
    "    for slot in slots:\n",
    "        out = np.zeros(n)\n",
    "        out = list(out)\n",
    "        out[slot_dict[slot]] = 1\n",
    "        outs.append(out) \n",
    "    y_train_slot.append(outs)\n",
    "    \n",
    "y_test_slot = list()\n",
    "for utterance in raw_test:\n",
    "    outs = list()\n",
    "    slots = utterance.split('\\t')[1].strip().split(' ')[:-1]\n",
    "    for slot in slots:\n",
    "        out = np.zeros(n)\n",
    "        out = list(out)\n",
    "        out[slot_dict[slot]] = 1\n",
    "        outs.append(out) \n",
    "    y_test_slot.append(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 127)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_train_slot[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make POS_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = dict()\n",
    "i = 0\n",
    "\n",
    "for utterance in train + test:\n",
    "    tags = nltk.pos_tag(utterance.split(' '))\n",
    "    for tag in tags:\n",
    "        tag = tag[1]\n",
    "        if(tag not in pos_dict):\n",
    "            pos_dict[tag] = i\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CC': 5,\n",
       " 'CD': 21,\n",
       " 'DT': 6,\n",
       " 'EX': 22,\n",
       " 'FW': 28,\n",
       " 'IN': 4,\n",
       " 'JJ': 9,\n",
       " 'JJR': 26,\n",
       " 'JJS': 12,\n",
       " 'MD': 15,\n",
       " 'NN': 0,\n",
       " 'NNP': 25,\n",
       " 'NNS': 8,\n",
       " 'PDT': 20,\n",
       " 'PRP': 14,\n",
       " 'PRP$': 27,\n",
       " 'RB': 17,\n",
       " 'RBR': 29,\n",
       " 'RBS': 19,\n",
       " 'RP': 24,\n",
       " 'TO': 2,\n",
       " 'UH': 30,\n",
       " 'VB': 3,\n",
       " 'VBD': 23,\n",
       " 'VBG': 11,\n",
       " 'VBN': 13,\n",
       " 'VBP': 1,\n",
       " 'VBZ': 10,\n",
       " 'WDT': 16,\n",
       " 'WP': 7,\n",
       " 'WRB': 18}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(pos_dict)\n",
    "max_len = 46\n",
    "\n",
    "x_train_pos = list()\n",
    "for utterance in train:\n",
    "    outs = np.zeros((max_len,len(pos_dict)))\n",
    "#     outs = list(outs)\n",
    "    tags = nltk.pos_tag(utterance.split(' '))\n",
    "    for i in range(len(tags)):\n",
    "        outs[i][pos_dict[tags[i][1]]] = 1\n",
    "    x_train_pos.append(outs)\n",
    "    \n",
    "x_test_pos = list()\n",
    "for utterance in test:\n",
    "    outs = np.zeros((max_len,len(pos_dict)))\n",
    "#     outs = list(outs)\n",
    "    tags = nltk.pos_tag(utterance.split(' '))\n",
    "    for i in range(len(tags)):\n",
    "        outs[i][pos_dict[tags[i][1]]] = 1\n",
    "    x_test_pos.append(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cheapest airfare from tacoma to orlando'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pos[3][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import os, pickle, sys, json, random, math \n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "#from  keras.np_utils import probas_to_classes\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Conv1D,Conv2D, MaxPooling1D,MaxPooling2D, Embedding, LSTM,GRU, add, concatenate, TimeDistributed, Bidirectional,Reshape,Bidirectional\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from keras.layers import Dense, Input, Flatten, Merge, Dropout, concatenate, Concatenate\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('/home/sinchan/Videos/slot_filling/embedding_matrix_m.pickle', 'r+') as fp:\n",
    "    embedding_matrix = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "with open('/home/sinchan/Videos/slot_filling/embedding_matrix_mw.pickle', 'r+') as fp:\n",
    "    embedding_matrix_w = pickle.load(fp)\n",
    "fp.close()\n",
    "with open('/home/sinchan/Videos/slot_filling/embedding_matrix_mg.pickle', 'r+') as fp:\n",
    "    embedding_matrix_g = pickle.load(fp)\n",
    "fp.close()\n",
    "with open('/home/sinchan/Videos/slot_filling/train_sequence.pickle', 'r+') as fp:\n",
    "    train_sequence = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "with open('/home/sinchan/Videos/slot_filling/test_sequence.pickle', 'r+') as fp:\n",
    "    test_sequence = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "with open('/home/sinchan/Videos/slot_filling/slot_word_dict.pickle', 'r+') as fp:\n",
    "    slot_word_dict = pickle.load(fp)\n",
    "fp.close()\n",
    "train_sequence = pad_sequences(train_sequence, maxlen = 46, padding='post')\n",
    "test_sequence = pad_sequences(test_sequence, maxlen = 46, padding='post') \n",
    "\n",
    "max_len = 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(slot_dict)\n",
    "\n",
    "y_train_slot = list()\n",
    "for utterance in raw_train:\n",
    "    outs = np.zeros((max_len,len(slot_dict)))\n",
    "    outs = list(outs)\n",
    "    slots = utterance.split('\\t')[1].strip().split(' ')[1:-1]\n",
    "    for i in range(len(slots)):\n",
    "        outs[i][slot_dict[slots[i]]] = 1\n",
    "    y_train_slot.append(outs)\n",
    "    \n",
    "y_test_slot = list()\n",
    "for utterance in raw_test:\n",
    "    outs = np.zeros((max_len,len(slot_dict)))\n",
    "    outs = list(outs)\n",
    "    slots = utterance.split('\\t')[1].strip().split(' ')[1:-1]\n",
    "    for i in range(len(slots)):\n",
    "        outs[i][slot_dict[slots[i]]] = 1\n",
    "    y_test_slot.append(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4978, 46, 127)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_train_slot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(slot_dict)\n",
    "\n",
    "z_train_slot = list()\n",
    "for utterance in raw_train:\n",
    "    outs = list()\n",
    "    slots = utterance.split('\\t')[1].strip().split(' ')[1:-1]\n",
    "    for i in range(len(slots)):\n",
    "        outs.append(slot_dict[slots[i]])\n",
    "    z_train_slot.append(outs)\n",
    "    \n",
    "z_test_slot = list()\n",
    "for utterance in raw_test:\n",
    "    outs = list()\n",
    "    slots = utterance.split('\\t')[1].strip().split(' ')[1:-1]\n",
    "    for i in range(len(slots)):\n",
    "        outs.append(slot_dict[slots[i]])\n",
    "    z_test_slot.append(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4978,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(z_train_slot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_z_pred_slot(mat,z_act_slot):\n",
    "    ans = list()\n",
    "    for i in range(len(mat)):\n",
    "        ans.append(mat[i][:len(z_act_slot[i])])\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accu(list1, list2):\n",
    "    if(len(list1) != len(list2)):\n",
    "        print(\"Size of a the lists not equal\")\n",
    "        return\n",
    "    count = 0.0\n",
    "    for i in range(len(list1)):\n",
    "        if(list1[i] == list2[i]):\n",
    "            count = count + 1\n",
    "            \n",
    "    return count/len(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(mat):\n",
    "    arr = list()\n",
    "    for ar in mat:\n",
    "        for a in ar:\n",
    "            arr.append(a)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_train_intent = list()\n",
    "z_test_intent = list()\n",
    "\n",
    "for out in y_train_label:\n",
    "    for i in range(len(out)):\n",
    "        if(out[i] == 1):\n",
    "            z_train_intent.append(i)\n",
    "        \n",
    "for out in y_test_label:\n",
    "    for i in range(len(out)):\n",
    "        if(out[i] == 1):\n",
    "            z_test_intent.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[y_test_label,y_test_slot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 893)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Label+Slot as output) GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 46)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 46, 300)      230100      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 46, 400)      601200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 23, 400)      0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 46, 400)      601200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 9200)         0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "slotdense (Dense)               (None, 46, 127)      50927       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "intentdense (Dense)             (None, 17)           156417      flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,639,844\n",
      "Trainable params: 1,409,744\n",
      "Non-trainable params: 230,100\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gru_embedding_layer = Embedding(len(embedding_matrix_g), len(embedding_matrix_g[0]), weights=[embedding_matrix_g], input_length=max_len, \n",
    "                            trainable=False, mask_zero = False)\n",
    "\n",
    "sequence_input = Input(shape=(max_len,), dtype='int32')\n",
    "\n",
    "gru_embedded_sequences = gru_embedding_layer(sequence_input)\n",
    "\n",
    "gru=Bidirectional(GRU(200, return_sequences=True))(gru_embedded_sequences)\n",
    "#gru=GRU(200,return_sequences=True,go_backwards=True)(gru)\n",
    "gru1=Bidirectional(GRU(200, return_sequences=True))(gru_embedded_sequences)\n",
    "#gru1=GRU(200,return_sequences=True,go_backwards=True)(gru1)\n",
    "pool=MaxPooling1D(2)(gru1)\n",
    "pool=Flatten()(pool)\n",
    "out1 = Dense(name='slotdense',units=len(slot_dict), activation='softmax', kernel_initializer='he_normal')(gru)\n",
    "out2 = Dense(name='intentdense',units=len(label_dict), activation='softmax', kernel_initializer='he_normal')(pool)\n",
    "graph2 = Model(inputs=sequence_input, outputs=[out1,out2])\n",
    "graph2.summary()\n",
    "graph2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveBestModel =ModelCheckpoint(filepath=\"/home/sinchan/Videos/slot_filling/s_lstm.hdf5\", monitor='val_slotdense_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "saveBestModel1=ModelCheckpoint(filepath=\"/home/sinchan/Videos/slot_filling/lab_lstm.hdf5\", monitor='val_intentdense_acc', verbose=1, save_best_only=True, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4978 samples, validate on 893 samples\n",
      "Epoch 1/10\n",
      " - 99s - loss: 0.7628 - slotdense_loss: 0.1988 - intentdense_loss: 0.5640 - slotdense_acc: 0.8817 - slotdense_categorical_accuracy: 0.8817 - intentdense_acc: 0.8578 - intentdense_categorical_accuracy: 0.8578 - val_loss: 0.3641 - val_slotdense_loss: 0.0713 - val_intentdense_loss: 0.2928 - val_slotdense_acc: 0.9016 - val_slotdense_categorical_accuracy: 0.9016 - val_intentdense_acc: 0.9216 - val_intentdense_categorical_accuracy: 0.9216\n",
      "\n",
      "Epoch 00001: val_slotdense_acc improved from -inf to 0.90155, saving model to /home/sinchan/Videos/slot_filling/s_lstm.hdf5\n",
      "\n",
      "Epoch 00001: val_intentdense_acc improved from -inf to 0.92161, saving model to /home/sinchan/Videos/slot_filling/lab_lstm.hdf5\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-ee9062e1c2b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgraph2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_slot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_slot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msaveBestModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msaveBestModel1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/sinchan/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/sinchan/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sinchan/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sinchan/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sinchan/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sinchan/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sinchan/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sinchan/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sinchan/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "graph2.fit(np.array(train_sequence), [np.array(y_train_slot),np.array(y_train_label)],validation_data=(np.array(test_sequence),[np.array(y_test_slot),np.array(y_test_label)]), epochs=10, batch_size=50, verbose=2, callbacks=[saveBestModel,saveBestModel1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph2.load_weights(\"/home/sinchan/Videos/slot_filling/s_lstm.hdf5\")\n",
    "a=graph2.predict(train_sequence)\n",
    "b=graph2.predict(test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4978, 17)\n",
      "(893, 17)\n",
      "(4978, 46, 127)\n",
      "(893, 46, 127)\n"
     ]
    }
   ],
   "source": [
    "gru_label_train = a[1]\n",
    "gru_label_test = b[1]\n",
    "gru_slot_train=a[0]\n",
    "gru_slot_test=b[0]\n",
    "#gru_label_train=np.array(gru_label_train)\n",
    "#gru_label_test=np.array(gru_label_test)\n",
    "#gru_slot_train=np.array(gru_slot_train)\n",
    "#gru_slot_test=np.array(gru_slot_test)\n",
    "print np.shape(gru_label_train)\n",
    "print np.shape(gru_label_test)\n",
    "print np.shape(gru_slot_train)\n",
    "print np.shape(gru_slot_test)\n",
    "gru_label_train=gru_label_train.argmax(axis=-1)\n",
    "gru_label_test=gru_label_test.argmax(axis=-1)\n",
    "gru_slot_train=gru_slot_train.argmax(axis=-1)\n",
    "gru_slot_test=gru_slot_test.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Train Accuracy: 99.9799116111%\n",
      "Label Test Accuracy: 97.8723404255%\n"
     ]
    }
   ],
   "source": [
    "print (\"Label Train Accuracy: \" + str(accu(z_train_intent,gru_label_train)*100) + \"%\")\n",
    "print (\"Label Test Accuracy: \" + str(accu(z_test_intent,gru_label_test)*100) + \"%\")                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_pred_slot_train = make_z_pred_slot(gru_slot_train,z_train_slot)\n",
    "z_pred_slot_test = make_z_pred_slot(gru_slot_test,z_test_slot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18,)"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(z_pred_slot_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1: 0.999338953602929\n",
      "Test F1: 0.9773221612364034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sinchan/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print (\"Train F1: \" + str(f1_score(flatten(z_train_slot),flatten(z_pred_slot_train), average = 'weighted')))\n",
    "print (\"Test F1: \" + str(f1_score(flatten(z_test_slot),flatten(z_pred_slot_test), average = 'weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slot Train Accuracy: 99.9359430605%\n",
      "Slot Test Accuracy: 97.9266695766%\n"
     ]
    }
   ],
   "source": [
    "print (\"Slot Train Accuracy: \" + str(accu(flatten(z_train_slot),flatten(z_pred_slot_train))*100) + \"%\")\n",
    "print (\"Slot Test Accuracy: \" + str(accu(flatten(z_test_slot),flatten(z_pred_slot_test))*100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BOS i want to fly from boston at 838 am and arrive in denver at 1110 in the morning EOS\\t O O O O O O B-fromloc.city_name O B-depart_time.time I-depart_time.time O O O B-toloc.city_name O B-arrive_time.time O O B-arrive_time.period_of_day atis_flight\\n',\n",
       " 'BOS what flights are available from pittsburgh to baltimore on thursday morning EOS\\tO O O O O O B-fromloc.city_name O B-toloc.city_name O B-depart_date.day_name B-depart_time.period_of_day atis_flight\\n',\n",
       " 'BOS what is the arrival time in san francisco for the 755 am flight leaving washington EOS\\tO O O O B-flight_time I-flight_time O B-fromloc.city_name I-fromloc.city_name O O B-depart_time.time I-depart_time.time O O B-fromloc.city_name atis_flight_time\\n',\n",
       " 'BOS cheapest airfare from tacoma to orlando EOS\\tO B-cost_relative O O B-fromloc.city_name O B-toloc.city_name atis_airfare\\n',\n",
       " 'BOS round trip fares from pittsburgh to philadelphia under 1000 dollars EOS\\tO B-round_trip I-round_trip O O B-fromloc.city_name O B-toloc.city_name B-cost_relative B-fare_amount I-fare_amount atis_airfare\\n',\n",
       " 'BOS i need a flight tomorrow from columbus to minneapolis EOS\\tO O O O O B-depart_date.today_relative O B-fromloc.city_name O B-toloc.city_name atis_flight\\n',\n",
       " 'BOS what kind of aircraft is used on a flight from cleveland to dallas EOS\\tO O O O O O O O O O O B-fromloc.city_name O B-toloc.city_name atis_aircraft\\n',\n",
       " 'BOS show me the flights from pittsburgh to los angeles on thursday EOS\\tO O O O O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name O B-depart_date.day_name atis_flight\\n',\n",
       " 'BOS all flights from boston to washington EOS\\tO O O O B-fromloc.city_name O B-toloc.city_name atis_flight\\n',\n",
       " 'BOS what kind of ground transportation is available in denver EOS\\tO O O O O O O O O B-city_name atis_ground_service\\n',\n",
       " 'BOS show me the flights from dallas to san francisco EOS\\tO O O O O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name atis_flight\\n']"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 2, 2, 0, 3, 0, 0, 4, 0])"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_label_train[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  1,  0,  2,  3,  0,  0,  0,  4,  0,  5,  0,\n",
       "         0,  6, 94, 94,  0,  0,  0,  0,  0,  0, 45, 45, 45, 45, 45, 45,\n",
       "        45, 45, 45, 45, 45, 45, 45, 45, 45,  5,  5,  5,  5,  5],\n",
       "       [ 0,  0,  0,  0,  0,  1,  0,  4,  0,  7,  8,  2,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 19],\n",
       "       [ 0,  0,  0,  9, 10,  0,  1, 11,  0,  0,  2,  3,  0,  0,  1, 34,\n",
       "         0, 78,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 74],\n",
       "       [12,  0,  0,  1,  0,  4, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 19],\n",
       "       [13, 14,  0,  0,  1,  0,  4, 12, 15, 16,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 19],\n",
       "       [ 0,  0,  0,  0, 17,  0,  1,  0,  4,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 19],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  4, 18,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 19],\n",
       "       [ 0,  0,  0,  0,  0,  1,  0,  4, 18,  0,  7, 36, 36, 36, 78, 78,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 19],\n",
       "       [ 0,  0,  0,  1,  0,  4, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 19],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0, 19, 55, 55, 55, 55, 55, 55, 55,\n",
       "        55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55,\n",
       "        55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55],\n",
       "       [ 0,  0,  0,  0,  0,  1,  0,  4, 18, 18, 39, 39, 39,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 19]])"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_slot_train[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 2\n",
    "TIME_STEPS = 46\n",
    "from keras.layers import Permute,merge\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "APPLY_ATTENTION_BEFORE_LSTM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs):\n",
    "    #inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    return output_attention_mul\n",
    "def model_attention_applied_after_lstm():\n",
    "    #inputs = Input(shape=(46,))\n",
    "    embedding_layer = Embedding(len(embedding_matrix_g), len(embedding_matrix_g[0]), weights=[embedding_matrix_g], input_length=max_len, \n",
    "                            trainable=False, mask_zero = False)\n",
    "    inputs = Input(shape=(max_len,), dtype='int32')\n",
    "    x = embedding_layer(inputs)\n",
    "    lstm_units = 32\n",
    "    lstm_out = LSTM(lstm_units, return_sequences=True)(x)\n",
    "    attention_mul = attention_3d_block(lstm_out)\n",
    "    #attention_mul = Flatten()(attention_mul)\n",
    "    output = Dense(name='slotdense',units=len(slot_dict), activation='softmax', kernel_initializer='he_normal')(attention_mul)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "def model_attention_applied_before_lstm():\n",
    "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "    attention_mul = attention_3d_block(inputs)\n",
    "    lstm_units = 32\n",
    "    attention_mul = LSTM(lstm_units, return_sequences=False)(attention_mul)\n",
    "    output = Dense(len(slot_dict), activation='sigmoid')(attention_mul)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sinchan/.local/lib/python2.7/site-packages/ipykernel_launcher.py:11: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/sinchan/.local/lib/python2.7/site-packages/keras/legacy/layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/home/sinchan/.local/lib/python2.7/site-packages/ipykernel_launcher.py:24: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"sl..., inputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "m = model_attention_applied_after_lstm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 46)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 46, 300)      230100      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 46, 32)       42624       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 32, 46)       0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 32, 46)       0           permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32, 46)       2162        reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 46, 32)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_mul (Merge)           (None, 46, 32)       0           lstm_1[0][0]                     \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "slotdense (Dense)               (None, 46, 127)      4191        attention_mul[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 279,077\n",
      "Trainable params: 48,977\n",
      "Non-trainable params: 230,100\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4978 samples, validate on 893 samples\n",
      "Epoch 1/10\n",
      " - 21s - loss: 0.5165 - acc: 0.1665 - categorical_accuracy: 0.1665 - val_loss: 0.4452 - val_acc: 0.1625 - val_categorical_accuracy: 0.1625\n",
      "Epoch 2/10\n",
      " - 22s - loss: 0.4770 - acc: 0.1834 - categorical_accuracy: 0.1834 - val_loss: 0.4166 - val_acc: 0.1850 - val_categorical_accuracy: 0.1850\n",
      "Epoch 3/10\n",
      " - 20s - loss: 0.4416 - acc: 0.1958 - categorical_accuracy: 0.1958 - val_loss: 0.3910 - val_acc: 0.1981 - val_categorical_accuracy: 0.1981\n",
      "Epoch 4/10\n",
      " - 21s - loss: 0.4116 - acc: 0.2062 - categorical_accuracy: 0.2062 - val_loss: 0.3682 - val_acc: 0.2003 - val_categorical_accuracy: 0.2003\n",
      "Epoch 5/10\n",
      " - 21s - loss: 0.3859 - acc: 0.2054 - categorical_accuracy: 0.2054 - val_loss: 0.3486 - val_acc: 0.2004 - val_categorical_accuracy: 0.2004\n",
      "Epoch 6/10\n",
      " - 19s - loss: 0.3627 - acc: 0.2213 - categorical_accuracy: 0.2213 - val_loss: 0.3312 - val_acc: 0.2263 - val_categorical_accuracy: 0.2263\n",
      "Epoch 7/10\n",
      " - 20s - loss: 0.3428 - acc: 0.2350 - categorical_accuracy: 0.2350 - val_loss: 0.3165 - val_acc: 0.2257 - val_categorical_accuracy: 0.2257\n",
      "Epoch 8/10\n",
      " - 19s - loss: 0.3261 - acc: 0.2374 - categorical_accuracy: 0.2374 - val_loss: 0.3034 - val_acc: 0.2265 - val_categorical_accuracy: 0.2265\n",
      "Epoch 9/10\n",
      " - 19s - loss: 0.3116 - acc: 0.2400 - categorical_accuracy: 0.2400 - val_loss: 0.2915 - val_acc: 0.2349 - val_categorical_accuracy: 0.2349\n",
      "Epoch 10/10\n",
      " - 19s - loss: 0.2985 - acc: 0.2475 - categorical_accuracy: 0.2475 - val_loss: 0.2810 - val_acc: 0.2356 - val_categorical_accuracy: 0.2356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0b05cc6c90>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(np.array(train_sequence), np.array(y_train_slot),validation_data=(np.array(test_sequence),np.array(y_test_slot)), epochs=10, batch_size=50, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import os\n",
    "\n",
    "def set_keras_backend(backend):\n",
    "\n",
    "    if K.backend() != backend:\n",
    "        os.environ['KERAS_BACKEND'] = backend\n",
    "        reload(K)\n",
    "        assert K.backend() == backend\n",
    "\n",
    "set_keras_backend(\"theano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers \n",
    "import numpy as np\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializers.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        self.W = self.add_weight(name='kernel', \n",
    "                                  shape=(input_shape[-1],),\n",
    "                                  initializer='normal',\n",
    "                                  trainable=True)\n",
    "        super(AttLayer, self).build(input_shape) \n",
    "     \n",
    "\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        print input_shape[0]\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(len(embedding_matrix_g), len(embedding_matrix_g[0]), weights=[embedding_matrix_g], input_length=max_len, \n",
    "                            trainable=False, mask_zero = False)\n",
    "sequence_input = Input(shape=(max_len,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_gru = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "l_att = AttLayer()(l_gru)\n",
    "preds = Dense(len(label_dict), activation='softmax')(l_att)\n",
    "model = Model(sequence_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_24 (InputLayer)        (None, 46)                0         \n",
      "_________________________________________________________________\n",
      "embedding_24 (Embedding)     (None, 46, 300)           230100    \n",
      "_________________________________________________________________\n",
      "bidirectional_25 (Bidirectio (None, 46, 200)           240600    \n",
      "_________________________________________________________________\n",
      "att_layer_24 (AttLayer)      (None, 200)               200       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 17)                3417      \n",
      "=================================================================\n",
      "Total params: 474,317\n",
      "Trainable params: 244,217\n",
      "Non-trainable params: 230,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy','categorical_accuracy'])\n",
    "model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc','categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4978 samples, validate on 893 samples\n",
      "Epoch 1/10\n",
      " - 42s - loss: 0.7140 - acc: 0.8301 - categorical_accuracy: 0.8301 - val_loss: 0.5153 - val_acc: 0.8936 - val_categorical_accuracy: 0.8936\n",
      "Epoch 2/10\n",
      " - 45s - loss: 0.2313 - acc: 0.9454 - categorical_accuracy: 0.9454 - val_loss: 0.2789 - val_acc: 0.9104 - val_categorical_accuracy: 0.9104\n",
      "Epoch 3/10\n",
      " - 44s - loss: 0.1272 - acc: 0.9691 - categorical_accuracy: 0.9691 - val_loss: 0.1844 - val_acc: 0.9518 - val_categorical_accuracy: 0.9518\n",
      "Epoch 4/10\n",
      " - 55s - loss: 0.0739 - acc: 0.9825 - categorical_accuracy: 0.9825 - val_loss: 0.1480 - val_acc: 0.9619 - val_categorical_accuracy: 0.9619\n",
      "Epoch 5/10\n",
      " - 45s - loss: 0.0509 - acc: 0.9885 - categorical_accuracy: 0.9885 - val_loss: 0.0952 - val_acc: 0.9754 - val_categorical_accuracy: 0.9754\n",
      "Epoch 6/10\n",
      " - 42s - loss: 0.0320 - acc: 0.9924 - categorical_accuracy: 0.9924 - val_loss: 0.0766 - val_acc: 0.9776 - val_categorical_accuracy: 0.9776\n",
      "Epoch 7/10\n",
      " - 51s - loss: 0.0220 - acc: 0.9948 - categorical_accuracy: 0.9948 - val_loss: 0.0776 - val_acc: 0.9798 - val_categorical_accuracy: 0.9798\n",
      "Epoch 8/10\n",
      " - 45s - loss: 0.0158 - acc: 0.9964 - categorical_accuracy: 0.9964 - val_loss: 0.0810 - val_acc: 0.9843 - val_categorical_accuracy: 0.9843\n",
      "Epoch 9/10\n",
      " - 43s - loss: 0.0112 - acc: 0.9970 - categorical_accuracy: 0.9970 - val_loss: 0.0852 - val_acc: 0.9821 - val_categorical_accuracy: 0.9821\n",
      "Epoch 10/10\n",
      " - 43s - loss: 0.0087 - acc: 0.9976 - categorical_accuracy: 0.9976 - val_loss: 0.0815 - val_acc: 0.9832 - val_categorical_accuracy: 0.9832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0ab85f1c90>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.array(train_sequence), np.array(y_train_label),validation_data=(np.array(test_sequence),np.array(y_test_label)), epochs=10, batch_size=50, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=model.predict(train_sequence)\n",
    "b=model.predict(test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_label_train = a\n",
    "gru_label_test = b\n",
    "gru_label_train=gru_label_train.argmax(axis=-1)\n",
    "gru_label_test=gru_label_test.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
