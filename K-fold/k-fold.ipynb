{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = []\n",
    "with open('ATIS_train.iob','r') as f:\n",
    "    for line in f:\n",
    "        raw_train.append(line)\n",
    "        \n",
    "raw_test = []\n",
    "with open('ATIS_test.iob','r') as f:\n",
    "    for line in f:\n",
    "        raw_test.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(893,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(raw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine=raw_train+raw_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "replace_dic = {'\\'d' : \"would\" , '\\'s' : \"is\" , '\\'t' : \"not\"}\n",
    "for sentence in raw_train:\n",
    "    sentence = sentence.split('\\t')[0][4:-4]\n",
    "    words = sentence.split(' ')\n",
    "    new_sentence = \"\"\n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        word = word.split(\"'\")[0]\n",
    "        if(word == ''):\n",
    "            print word\n",
    "            continue;\n",
    "        if(word in replace_dic):\n",
    "            print(word)\n",
    "            word = replace_dic[word];\n",
    "        if(word.isdigit()):\n",
    "            word = \"number\"\n",
    "        new_sentence = new_sentence + \" \" + word\n",
    "    train.append(new_sentence[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-418437b35e62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'raw' is not defined"
     ]
    }
   ],
   "source": [
    "np.shape(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "replace_dic = {'\\'d' : \"would\" , '\\'s' : \"is\" , '\\'t' : \"not\"}\n",
    "for sentence in raw_test:\n",
    "    sentence = sentence.split('\\t')[0][4:-4]\n",
    "    words = sentence.split(' ')\n",
    "    new_sentence = \"\"\n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        word = word.split(\"'\")[0]\n",
    "        if(word == ''):\n",
    "            continue;\n",
    "        if(word in replace_dic):\n",
    "            word = replace_dic[word];\n",
    "        if(word.isdigit()):\n",
    "            word = \"number\"\n",
    "        new_sentence = new_sentence + \" \" + word\n",
    "    test.append(new_sentence[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/sinchan/Videos/slot_filling/train_data.pickle', 'w+') as fp:\n",
    "     pickle.dump(train,fp)\n",
    "fp.close()\n",
    "\n",
    "with open('/home/sinchan/Videos/slot_filling/test_data.pickle', 'w+') as fp:\n",
    "     pickle.dump(test,fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/sinchan/Videos/slot_filling/train_data.pickle', 'r+') as fp:\n",
    "    train = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "with open('/home/sinchan/Videos/slot_filling/test_data.pickle', 'r+') as fp:\n",
    "    test = pickle.load(fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i would like to find a flight from charlotte to las vegas that makes a stop in st. louis'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forK-fold\n",
    "combine=test+train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5871"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BOS i would like to find a flight from charlotte to las vegas that makes a stop in st. louis EOS\\tO O O O O O O O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name O O O O O B-stoploc.city_name I-stoploc.city_name atis_flight\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = raw_train + raw_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(word):\n",
    "    replace_dic = {'\\'d' : \"would\" , '\\'s' : \"is\" , '\\'t' : \"not\"}\n",
    "    word = word.split(\"'\")[0]\n",
    "    if(word == ''):\n",
    "        return word\n",
    "    if(word in replace_dic):\n",
    "        word = replace_dic[word];\n",
    "    if(word.isdigit()):\n",
    "        word = \"number\"\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot_dict = dict()\n",
    "word_dict = dict()\n",
    "\n",
    "i_s = 0\n",
    "i_w = 1\n",
    "a=[[]]\n",
    "ii=0;\n",
    "for utterance in raw:\n",
    "    j=1\n",
    "    ii=ii+1\n",
    "    t = utterance.split('\\t')\n",
    "    words = t[0].strip().split(' ')\n",
    "    slots = t[1].strip().split(' ')\n",
    "    n = len(words)\n",
    "    for i in xrange(n):\n",
    "        word = check(words[i])\n",
    "        if(word not in word_dict):\n",
    "            word_dict[word] = i_w\n",
    "            i_w = i_w + 1\n",
    "    for i in xrange(n-1):\n",
    "        if(slots[i] not in slot_dict):\n",
    "            slot_dict[slots[i]] = i_s\n",
    "            i_s = i_s + 1\n",
    "        j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(slot_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = [slot_dict,word_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/sinchan/Videos/slot_filling/slot_word_dict.pickle', 'w+') as fp:\n",
    "    pickle.dump(dicts,fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/sinchan/Videos/slot_filling/slot_word_dict.pickle', 'r+') as fp:\n",
    "    dicts = pickle.load(fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot_dict,word_dict = dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(slot_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences(data,word_dict):\n",
    "    sequences = list()\n",
    "    for sentence in data:\n",
    "        sequence = list()\n",
    "        words = sentence.split(' ')\n",
    "        for word in words:\n",
    "                sequence.append(word_dict[word])\n",
    "        sequences.append(sequence)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine=test+train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine=np.array(combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = make_sequences(train,word_dict)\n",
    "test_sequence = make_sequences(test,word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4978,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/sinchan/Videos/slot_filling/train_sequence.pickle', 'w+') as fp:\n",
    "     pickle.dump(train_sequence,fp)\n",
    "fp.close()\n",
    "\n",
    "with open('/home/sinchan/Videos/slot_filling/test_sequence.pickle', 'w+') as fp:\n",
    "     pickle.dump(test_sequence,fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/sinchan/Videos/slot_filling/train_sequence.pickle', 'r+') as fp:\n",
    "    train_sequence = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "with open('/home/sinchan/Videos/slot_filling/test_sequence.pickle', 'r+') as fp:\n",
    "    test_sequence = pickle.load(fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences=train_sequence+test_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "m = 0\n",
    "lens = list()\n",
    "for sequence in sequences:\n",
    "    lens.append(len(sequence))\n",
    "    m = max(len(sequence),m)\n",
    "    \n",
    "print m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f82c0a01890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.hist(lens,46)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Output in One-Hot form(Slot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BOS find me a flight that flies from memphis to tacoma EOS',\n",
       " 'O O O O O O O O B-fromloc.city_name O B-toloc.city_name atis_flight\\n']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterance = raw_train[0]\n",
    "slots = utterance.split('\\t')[1].strip().split(' ')[:-1]\n",
    "slots = t\n",
    "slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(slot_dict)\n",
    "\n",
    "y_train_slot = list()\n",
    "for utterance in raw_train:\n",
    "    outs = list()\n",
    "    slots = utterance.split('\\t')[1].strip().split(' ')[:-1]\n",
    "    for slot in slots:\n",
    "        out = np.zeros(n)\n",
    "        out = list(out)\n",
    "        out[slot_dict[slot]] = 1\n",
    "        outs.append(out) \n",
    "    y_train_slot.append(outs)\n",
    "    \n",
    "y_test_slot = list()\n",
    "for utterance in raw_test:\n",
    "    outs = list()\n",
    "    slots = utterance.split('\\t')[1].strip().split(' ')[:-1]\n",
    "    for slot in slots:\n",
    "        out = np.zeros(n)\n",
    "        out = list(out)\n",
    "        out[slot_dict[slot]] = 1\n",
    "        outs.append(out) \n",
    "    y_test_slot.append(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4978,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_train_slot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-e867e6726007>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_val' is not defined"
     ]
    }
   ],
   "source": [
    "len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combine_slot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-9f9e98ec1d5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombine_slot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'combine_slot' is not defined"
     ]
    }
   ],
   "source": [
    "len(combine_slot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make POS_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = dict()\n",
    "i = 0\n",
    "\n",
    "for utterance in train + test:\n",
    "    tags = nltk.pos_tag(utterance.split(' '))\n",
    "    for tag in tags:\n",
    "        tag = tag[1]\n",
    "        if(tag not in pos_dict):\n",
    "            pos_dict[tag] = i\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CC': 5,\n",
       " 'CD': 21,\n",
       " 'DT': 6,\n",
       " 'EX': 22,\n",
       " 'FW': 28,\n",
       " 'IN': 4,\n",
       " 'JJ': 9,\n",
       " 'JJR': 26,\n",
       " 'JJS': 12,\n",
       " 'MD': 15,\n",
       " 'NN': 0,\n",
       " 'NNP': 25,\n",
       " 'NNS': 8,\n",
       " 'PDT': 20,\n",
       " 'PRP': 14,\n",
       " 'PRP$': 27,\n",
       " 'RB': 17,\n",
       " 'RBR': 29,\n",
       " 'RBS': 19,\n",
       " 'RP': 24,\n",
       " 'TO': 2,\n",
       " 'UH': 30,\n",
       " 'VB': 3,\n",
       " 'VBD': 23,\n",
       " 'VBG': 11,\n",
       " 'VBN': 13,\n",
       " 'VBP': 1,\n",
       " 'VBZ': 10,\n",
       " 'WDT': 16,\n",
       " 'WP': 7,\n",
       " 'WRB': 18}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(pos_dict)\n",
    "max_len = 46\n",
    "\n",
    "x_train_pos = list()\n",
    "for utterance in train:\n",
    "    outs = np.zeros((max_len,len(pos_dict)))\n",
    "#     outs = list(outs)\n",
    "    tags = nltk.pos_tag(utterance.split(' '))\n",
    "    for i in range(len(tags)):\n",
    "        outs[i][pos_dict[tags[i][1]]] = 1\n",
    "    x_train_pos.append(outs)\n",
    "    \n",
    "x_test_pos = list()\n",
    "for utterance in test:\n",
    "    outs = np.zeros((max_len,len(pos_dict)))\n",
    "#     outs = list(outs)\n",
    "    tags = nltk.pos_tag(utterance.split(' '))\n",
    "    for i in range(len(tags)):\n",
    "        outs[i][pos_dict[tags[i][1]]] = 1\n",
    "    x_test_pos.append(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4978, 46, 31)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4978,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_train_slot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pos[3][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences(data,word_dict):\n",
    "    sequences = list()\n",
    "    for sentence in data:\n",
    "        sequence = list()\n",
    "        words = sentence.split(' ')\n",
    "        for word in words:\n",
    "                sequence.append(word_dict[word])\n",
    "        sequences.append(sequence)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "EXTRAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sinchan/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os, pickle, sys, json, random, math \n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "# from keras.np_utils import probas_to_classes\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Conv1D,Conv2D, MaxPooling1D,MaxPooling2D, Embedding, LSTM, add, concatenate, TimeDistributed, Bidirectional,Reshape\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from keras.layers import Dense, Input, Flatten, Merge, Dropout, concatenate, Concatenate\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('/home/sinchan/Videos/slot_filling/embedding_matrix_m.pickle', 'r+') as fp:\n",
    "    embedding_matrix = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "with open('/home/sinchan/Videos/slot_filling/embedding_matrix_mw.pickle', 'r+') as fp:\n",
    "    embedding_matrix_w = pickle.load(fp)\n",
    "fp.close()\n",
    "with open('/home/sinchan/Videos/slot_filling/embedding_matrix_mg.pickle', 'r+') as fp:\n",
    "    embedding_matrix_g = pickle.load(fp)\n",
    "fp.close()\n",
    "with open('/home/sinchan/Videos/slot_filling/train_sequence.pickle', 'r+') as fp:\n",
    "    train_sequence = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "with open('/home/sinchan/Videos/slot_filling/test_sequence.pickle', 'r+') as fp:\n",
    "    test_sequence = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "with open('/home/sinchan/Videos/slot_filling/slot_word_dict.pickle', 'r+') as fp:\n",
    "    slot_word_dict = pickle.load(fp)\n",
    "fp.close()\n",
    "train_sequence = pad_sequences(train_sequence, maxlen = 46, padding='post')\n",
    "test_sequence = pad_sequences(test_sequence, maxlen = 46, padding='post') \n",
    "\n",
    "max_len = 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(893, 46)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(slot_dict)\n",
    "\n",
    "y_train_slot = list()\n",
    "for utterance in raw_train:\n",
    "    outs = np.zeros((max_len,len(slot_dict)))\n",
    "    outs = list(outs)\n",
    "    slots = utterance.split('\\t')[1].strip().split(' ')[1:-1]\n",
    "    for i in range(len(slots)):\n",
    "        outs[i][slot_dict[slots[i]]] = 1\n",
    "    y_train_slot.append(outs)\n",
    "    \n",
    "y_test_slot = list()\n",
    "for utterance in raw_test:\n",
    "    outs = np.zeros((max_len,len(slot_dict)))\n",
    "    outs = list(outs)\n",
    "    slots = utterance.split('\\t')[1].strip().split(' ')[1:-1]\n",
    "    for i in range(len(slots)):\n",
    "        outs[i][slot_dict[slots[i]]] = 1\n",
    "    y_test_slot.append(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4978, 46, 127)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_train_slot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(slot_dict)\n",
    "\n",
    "z_train_slot = list()\n",
    "for utterance in raw_train:\n",
    "    outs = list()\n",
    "    slots = utterance.split('\\t')[1].strip().split(' ')[1:-1]\n",
    "    for i in range(len(slots)):\n",
    "        outs.append(slot_dict[slots[i]])\n",
    "    z_train_slot.append(outs)\n",
    "    \n",
    "z_test_slot = list()\n",
    "for utterance in raw_test:\n",
    "    outs = list()\n",
    "    slots = utterance.split('\\t')[1].strip().split(' ')[1:-1]\n",
    "    for i in range(len(slots)):\n",
    "        outs.append(slot_dict[slots[i]])\n",
    "    z_test_slot.append(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(mat):\n",
    "    arr = list()\n",
    "    for ar in mat:\n",
    "        for a in ar:\n",
    "            arr.append(a)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_z_pred_slot(mat,z_act_slot):\n",
    "    ans = list()\n",
    "    for i in range(len(mat)):\n",
    "        ans.append(mat[i][:len(z_act_slot[i])])\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accu(list1, list2):\n",
    "    if(len(list1) != len(list2)):\n",
    "        print(\"Size of a the lists not equal\")\n",
    "        return\n",
    "    count = 0.0\n",
    "    for i in range(len(list1)):\n",
    "        if(list1[i] == list2[i]):\n",
    "            count = count + 1\n",
    "            \n",
    "    return count/len(list1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_test_slot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 127 and 46 for 'loss/dense_2_loss/mul_1' (op: 'Mul') with input shapes: [?,127], [?,46].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-d2dd1c448c21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'categorical_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/sinchan/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    828\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m                     output_loss = weighted_loss(y_true, y_pred,\n\u001b[0;32m--> 830\u001b[0;31m                                                 sample_weight, mask)\n\u001b[0m\u001b[1;32m    831\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sinchan/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mweighted\u001b[0;34m(y_true, y_pred, weights, mask)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;31m# mask should have the same shape as score_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mscore_array\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m             \u001b[0;31m#  the loss per batch should be proportional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;31m#  to the number of unmasked samples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sinchan/.local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.pyc\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    977\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbinary_op_wrapper_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sinchan/.local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.pyc\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1209\u001b[0m   \u001b[0mis_tensor_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_tensor_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1212\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Case: Dense * Sparse.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sinchan/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.pyc\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   4757\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4758\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 4759\u001b[0;31m         \"Mul\", x=x, y=y, name=name)\n\u001b[0m\u001b[1;32m   4760\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4761\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sinchan/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sinchan/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3390\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3391\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3392\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3394\u001b[0m       \u001b[0;31m# Note: shapes are lazily computed with the C API enabled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sinchan/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1732\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1733\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1734\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1735\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1736\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sinchan/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1568\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 127 and 46 for 'loss/dense_2_loss/mul_1' (op: 'Mul') with input shapes: [?,127], [?,46]."
     ]
    }
   ],
   "source": [
    "lstm_embedding_layer = Embedding(len(embedding_matrix_g), len(embedding_matrix_g[0]), weights=[embedding_matrix_g], input_length=127, \n",
    "                            trainable=False, mask_zero = True)\n",
    "\n",
    "sequence_input = Input(shape=(max_len,), dtype='int32')\n",
    "\n",
    "lstm_embedded_sequences = lstm_embedding_layer(sequence_input)\n",
    "\n",
    "lstm = LSTM(200,return_sequences=True)(lstm_embedded_sequences)\n",
    "\n",
    "out = Dense(units=200, activation='relu', kernel_initializer='he_normal')(lstm)\n",
    "\n",
    "out = Dense(units=len(slot_dict), activation='softmax', kernel_initializer='he_normal')(out)\n",
    "\n",
    "graph = Model(inputs=sequence_input, outputs=out)\n",
    "graph.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_26 (Embedding)     (None, 46, 600)           460200    \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 46, 32)            57632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 23, 32)            0         \n",
      "_________________________________________________________________\n",
      "reshape_19 (Reshape)         (None, 46, 16)            0         \n",
      "_________________________________________________________________\n",
      "lstm_31 (LSTM)               (None, 46, 100)           46800     \n",
      "_________________________________________________________________\n",
      "lstm_32 (LSTM)               (None, 46, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 46, 300)           30300     \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 46, 127)           38227     \n",
      "=================================================================\n",
      "Total params: 713,559\n",
      "Trainable params: 253,359\n",
      "Non-trainable params: 460,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(embedding_matrix), len(embedding_matrix[0]), weights=[embedding_matrix], input_length=46, trainable=False))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Reshape((23*2,16)))\n",
    "model.add(LSTM(100, dropout=0.2, return_sequences=True))\n",
    "model.add(LSTM(100, dropout=0.2, return_sequences=True))\n",
    "model.add(Dense(300,activation='relu',kernel_initializer='he_normal'))\n",
    "model.add(Dense(len(slot_dict), activation=\"sigmoid\",kernel_initializer='he_normal',input_shape=(46,)))\n",
    "model.summary()\n",
    "model.save('rnncnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveBestModel = ModelCheckpoint(filepath=\"/home/sinchan/Videos/slot_filling/g_lstm.hdf5\", monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4978 samples, validate on 893 samples\n",
      "Epoch 1/10\n",
      " - 37s - loss: 0.6277 - acc: 0.9004 - categorical_accuracy: 0.9004 - val_loss: 0.4013 - val_acc: 0.9108 - val_categorical_accuracy: 0.9108\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91083, saving model to /home/sinchan/Videos/slot_filling/g_lstm.hdf5\n",
      "Epoch 2/10\n",
      " - 24s - loss: 0.3250 - acc: 0.9103 - categorical_accuracy: 0.9103 - val_loss: 0.2825 - val_acc: 0.9108 - val_categorical_accuracy: 0.9108\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.91083\n",
      "Epoch 3/10\n",
      " - 24s - loss: 0.2443 - acc: 0.8983 - categorical_accuracy: 0.8983 - val_loss: 0.2226 - val_acc: 0.7206 - val_categorical_accuracy: 0.7206\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.91083\n",
      "Epoch 4/10\n",
      " - 24s - loss: 0.1787 - acc: 0.2476 - categorical_accuracy: 0.2476 - val_loss: 0.1403 - val_acc: 0.1935 - val_categorical_accuracy: 0.1935\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.91083\n",
      "Epoch 5/10\n",
      " - 26s - loss: 0.1199 - acc: 0.2219 - categorical_accuracy: 0.2219 - val_loss: 0.1122 - val_acc: 0.1995 - val_categorical_accuracy: 0.1995\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.91083\n",
      "Epoch 6/10\n",
      " - 24s - loss: 0.0970 - acc: 0.2270 - categorical_accuracy: 0.2270 - val_loss: 0.0943 - val_acc: 0.2080 - val_categorical_accuracy: 0.2080\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.91083\n",
      "Epoch 7/10\n",
      " - 24s - loss: 0.0807 - acc: 0.2319 - categorical_accuracy: 0.2319 - val_loss: 0.0863 - val_acc: 0.2127 - val_categorical_accuracy: 0.2127\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.91083\n",
      "Epoch 8/10\n",
      " - 24s - loss: 0.0686 - acc: 0.2359 - categorical_accuracy: 0.2359 - val_loss: 0.0735 - val_acc: 0.2109 - val_categorical_accuracy: 0.2109\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.91083\n",
      "Epoch 9/10\n",
      " - 24s - loss: 0.0588 - acc: 0.2371 - categorical_accuracy: 0.2371 - val_loss: 0.0692 - val_acc: 0.2145 - val_categorical_accuracy: 0.2145\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.91083\n",
      "Epoch 10/10\n",
      " - 25s - loss: 0.0526 - acc: 0.2376 - categorical_accuracy: 0.2376 - val_loss: 0.0677 - val_acc: 0.2187 - val_categorical_accuracy: 0.2187\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.91083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f80baf933d0>"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.array(train_sequence), np.array(y_train_slot),validation_data=(np.array(test_sequence),np.array(y_test_slot)), epochs=10, batch_size=50, verbose=2,callbacks=[saveBestModel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred_train = model.predict(train_sequence)\n",
    "lstm_pred_test =  model.predict(test_sequence)\n",
    "lstm_pred_classes_train = lstm_pred_train.argmax(axis=-1)\n",
    "lstm_pred_classes_test = lstm_pred_test.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "thefile = open('/home/sinchan/Videos/predSlotTest.txt', 'w')\n",
    "thefile1= open('/home/sinchan/Videos/predSlotTrain.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in lstm_pred_train:\n",
    "  thefile.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_pred_slot_train = make_z_pred_slot(lstm_pred_classes_train,z_train_slot)\n",
    "z_pred_slot_test = make_z_pred_slot(lstm_pred_classes_test,z_test_slot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in z_pred_slot_train:\n",
    "  thefile.write(\"%s\\n\" % item)\n",
    "for item in z_pred_slot_test:\n",
    "  thefile1.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1: 0.9456337726026143\n",
      "Test F1: 0.920732482768037\n"
     ]
    }
   ],
   "source": [
    "print (\"Train F1: \" + str(f1_score(flatten(z_train_slot),flatten(z_pred_slot_train), average = 'weighted')))\n",
    "print (\"Test F1: \" + str(f1_score(flatten(z_test_slot),flatten(z_pred_slot_test), average = 'weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(len(embedding_matrix), len(embedding_matrix[0]), weights=[embedding_matrix], input_length=46, trainable=False))\n",
    "model2.add(Conv1D(filters=100, kernel_size=3, padding='same', activation='relu'))\n",
    "model2.add(MaxPooling1D(pool_size=2))\n",
    "model2.add(Reshape((23*2,50)))\n",
    "model2.add(Dense(300,activation='relu',kernel_initializer='he_normal'))\n",
    "model2.add(Dense(len(slot_dict), activation=\"sigmoid\",kernel_initializer='he_normal',input_shape=(46,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_27 (Embedding)     (None, 46, 600)           460200    \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 46, 100)           180100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 23, 100)           0         \n",
      "_________________________________________________________________\n",
      "reshape_20 (Reshape)         (None, 46, 50)            0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 46, 300)           15300     \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 46, 127)           38227     \n",
      "=================================================================\n",
      "Total params: 693,827\n",
      "Trainable params: 233,627\n",
      "Non-trainable params: 460,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveBestModel = ModelCheckpoint(filepath=\"/home/sinchan/Videos/slot_filling/cnne_lstm.hdf5\", monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4978 samples, validate on 893 samples\n",
      "Epoch 1/10\n",
      " - 16s - loss: 0.3468 - acc: 0.9078 - categorical_accuracy: 0.9078 - val_loss: 0.1477 - val_acc: 0.9108 - val_categorical_accuracy: 0.9108\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91083, saving model to /home/sinchan/Videos/slot_filling/cnne_lstm.hdf5\n",
      "Epoch 2/10\n",
      " - 13s - loss: 0.1144 - acc: 0.9103 - categorical_accuracy: 0.9103 - val_loss: 0.1159 - val_acc: 0.9110 - val_categorical_accuracy: 0.9110\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.91083 to 0.91105, saving model to /home/sinchan/Videos/slot_filling/cnne_lstm.hdf5\n",
      "Epoch 3/10\n",
      " - 13s - loss: 0.0907 - acc: 0.9164 - categorical_accuracy: 0.9164 - val_loss: 0.0628 - val_acc: 0.9634 - val_categorical_accuracy: 0.9634\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.91105 to 0.96344, saving model to /home/sinchan/Videos/slot_filling/cnne_lstm.hdf5\n",
      "Epoch 4/10\n",
      " - 13s - loss: 0.0293 - acc: 0.9713 - categorical_accuracy: 0.9713 - val_loss: 0.0476 - val_acc: 0.9663 - val_categorical_accuracy: 0.9663\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.96344 to 0.96633, saving model to /home/sinchan/Videos/slot_filling/cnne_lstm.hdf5\n",
      "Epoch 5/10\n",
      " - 13s - loss: 0.0206 - acc: 0.9757 - categorical_accuracy: 0.9757 - val_loss: 0.0441 - val_acc: 0.9708 - val_categorical_accuracy: 0.9708\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.96633 to 0.97084, saving model to /home/sinchan/Videos/slot_filling/cnne_lstm.hdf5\n",
      "Epoch 6/10\n",
      " - 13s - loss: 0.0172 - acc: 0.9768 - categorical_accuracy: 0.9768 - val_loss: 0.0441 - val_acc: 0.9703 - val_categorical_accuracy: 0.9703\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.97084\n",
      "Epoch 7/10\n",
      " - 13s - loss: 0.0151 - acc: 0.9755 - categorical_accuracy: 0.9755 - val_loss: 0.0438 - val_acc: 0.9664 - val_categorical_accuracy: 0.9664\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.97084\n",
      "Epoch 8/10\n",
      " - 14s - loss: 0.0136 - acc: 0.9760 - categorical_accuracy: 0.9760 - val_loss: 0.0481 - val_acc: 0.9667 - val_categorical_accuracy: 0.9667\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.97084\n",
      "Epoch 9/10\n",
      " - 13s - loss: 0.0129 - acc: 0.9754 - categorical_accuracy: 0.9754 - val_loss: 0.0487 - val_acc: 0.9718 - val_categorical_accuracy: 0.9718\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.97084 to 0.97183, saving model to /home/sinchan/Videos/slot_filling/cnne_lstm.hdf5\n",
      "Epoch 10/10\n",
      " - 13s - loss: 0.0117 - acc: 0.9750 - categorical_accuracy: 0.9750 - val_loss: 0.0493 - val_acc: 0.9676 - val_categorical_accuracy: 0.9676\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.97183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f80c29e5950>"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(np.array(train_sequence), np.array(y_train_slot),validation_data=(np.array(test_sequence),np.array(y_test_slot)), epochs=10, batch_size=50, verbose=2,callbacks=[saveBestModel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred_train = model2.predict(train_sequence)\n",
    "lstm_pred_test =  model2.predict(test_sequence)\n",
    "lstm_pred_classes_train = lstm_pred_train.argmax(axis=-1)\n",
    "lstm_pred_classes_test = lstm_pred_test.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_pred_slot_train = make_z_pred_slot(lstm_pred_classes_train,z_train_slot)\n",
    "z_pred_slot_test = make_z_pred_slot(lstm_pred_classes_test,z_test_slot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1: 0.9818047942394821\n",
      "Test F1: 0.9453120280255638\n"
     ]
    }
   ],
   "source": [
    "print (\"Train F1: \" + str(f1_score(flatten(z_train_slot),flatten(z_pred_slot_train), average = 'weighted')))\n",
    "print (\"Test F1: \" + str(f1_score(flatten(z_test_slot),flatten(z_pred_slot_test), average = 'weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 98.4021352313%\n",
      "Test Accuracy: 95.2422522916%\n"
     ]
    }
   ],
   "source": [
    "print (\"Train Accuracy: \" + str(accu(flatten(z_train_slot),flatten(z_pred_slot_train))*100) + \"%\")\n",
    "print (\"Test Accuracy: \" + str(accu(flatten(z_test_slot),flatten(z_pred_slot_test))*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sinchan/.local/lib/python2.7/site-packages/ipykernel_launcher.py:31: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=30, activation=\"relu\")`\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(len(embedding_matrix_w), len(embedding_matrix_w[0]), weights=[embedding_matrix_w], input_length=max_len, trainable=False)\n",
    "\n",
    "sequence_input = Input(shape=(max_len,), dtype='int32')\n",
    "\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "conv0 = Conv1D(filters=100, kernel_size=3, activation='relu', padding='same',kernel_initializer='he_normal')(embedded_sequences)\n",
    "pool0 = MaxPooling1D(2)(conv0)\n",
    "#flatten0 = Flatten()(pool0)\n",
    "\n",
    "conv1 = Conv1D(filters=100, kernel_size=3, activation='relu', padding='same',kernel_initializer='he_normal')(embedded_sequences)\n",
    "pool1 = MaxPooling1D(2)(conv1)\n",
    "#flatten1 = Flatten()(pool1)\n",
    "\n",
    "conv2 = Conv1D(filters=100, kernel_size=3, activation='relu', padding='same',kernel_initializer='he_normal')(embedded_sequences)\n",
    "pool2 = MaxPooling1D(2)(conv2)\n",
    "#flatten2 = Flatten()(pool2)\n",
    "\n",
    "# out = Merge(mode='concat')([flatten0,flatten1,flatten2])\n",
    "#out = concatenate([flatten0,flatten1,flatten2])\n",
    "out = concatenate([pool0,pool1,pool2])\n",
    "graph = Model(inputs=sequence_input, outputs=out)\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(graph)\n",
    "\n",
    "model3.add(Dropout(0.10))\n",
    "model3.add(Reshape((23*2,150)))\n",
    "model3.add(LSTM(100, dropout=0.2, return_sequences=True))\n",
    "model3.add(LSTM(100, dropout=0.2, return_sequences=True))\n",
    "model3.add(Dense(output_dim=30, activation='relu'))\n",
    "model3.add(Dense(units=len(slot_dict), activation='softmax',kernel_initializer='he_normal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_3 (Model)              (None, 23, 300)           500400    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 23, 300)           0         \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 46, 150)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 46, 100)           100400    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 46, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 46, 30)            3030      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 46, 127)           3937      \n",
      "=================================================================\n",
      "Total params: 688,167\n",
      "Trainable params: 458,067\n",
      "Non-trainable params: 230,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveBestModel1 = ModelCheckpoint(filepath=\"/home/sinchan/Videos/slot_filling/cnn_lstm3.hdf5\", monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4978 samples, validate on 893 samples\n",
      "Epoch 1/10\n",
      " - 39s - loss: 1.1702 - acc: 0.5074 - categorical_accuracy: 0.5074 - val_loss: 1.0467 - val_acc: 0.9108 - val_categorical_accuracy: 0.9108\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91083, saving model to /home/sinchan/Videos/slot_filling/cnn4_lstm.hdf5\n",
      "Epoch 2/10\n",
      " - 37s - loss: 1.1325 - acc: 0.9103 - categorical_accuracy: 0.9103 - val_loss: 1.0135 - val_acc: 0.9108 - val_categorical_accuracy: 0.9108\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.91083\n",
      "Epoch 3/10\n",
      " - 37s - loss: 1.0956 - acc: 0.9103 - categorical_accuracy: 0.9103 - val_loss: 0.9810 - val_acc: 0.9108 - val_categorical_accuracy: 0.9108\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.91083\n",
      "Epoch 4/10\n",
      " - 37s - loss: 1.0596 - acc: 0.9103 - categorical_accuracy: 0.9103 - val_loss: 0.9494 - val_acc: 0.9108 - val_categorical_accuracy: 0.9108\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.91083\n",
      "Epoch 5/10\n",
      " - 37s - loss: 1.0244 - acc: 0.9103 - categorical_accuracy: 0.9103 - val_loss: 0.9187 - val_acc: 0.9108 - val_categorical_accuracy: 0.9108\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.91083\n",
      "Epoch 6/10\n",
      " - 43s - loss: 0.9902 - acc: 0.9103 - categorical_accuracy: 0.9103 - val_loss: 0.8889 - val_acc: 0.9108 - val_categorical_accuracy: 0.9108\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.91083\n",
      "Epoch 7/10\n",
      " - 42s - loss: 0.9568 - acc: 0.9103 - categorical_accuracy: 0.9103 - val_loss: 0.8599 - val_acc: 0.9108 - val_categorical_accuracy: 0.9108\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.91083\n",
      "Epoch 8/10\n",
      " - 37s - loss: 0.9243 - acc: 0.9103 - categorical_accuracy: 0.9103 - val_loss: 0.8319 - val_acc: 0.9108 - val_categorical_accuracy: 0.9108\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.91083\n",
      "Epoch 9/10\n",
      " - 36s - loss: 0.8927 - acc: 0.9103 - categorical_accuracy: 0.9103 - val_loss: 0.8048 - val_acc: 0.9108 - val_categorical_accuracy: 0.9108\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.91083\n",
      "Epoch 10/10\n",
      " - 37s - loss: 0.8622 - acc: 0.9103 - categorical_accuracy: 0.9103 - val_loss: 0.7788 - val_acc: 0.9108 - val_categorical_accuracy: 0.9108\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.91083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fad06828590>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(np.array(train_sequence), np.array(y_train_slot),validation_data=(np.array(test_sequence),np.array(y_test_slot)), epochs=10, batch_size=50, verbose=2,callbacks=[saveBestModel1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred_train = model3.predict(train_sequence)\n",
    "lstm_pred_test =  model3.predict(test_sequence)\n",
    "lstm_pred_classes_train = lstm_pred_train.argmax(axis=-1)\n",
    "lstm_pred_classes_test = lstm_pred_test.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_pred_slot_train = make_z_pred_slot(lstm_pred_classes_train,z_train_slot)\n",
    "z_pred_slot_test = make_z_pred_slot(lstm_pred_classes_test,z_test_slot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1: 0.49284069196626834\n",
      "Test F1: 0.4503458217073495\n"
     ]
    }
   ],
   "source": [
    "print (\"Train F1: \" + str(f1_score(flatten(z_train_slot),flatten(z_pred_slot_train), average = 'weighted')))\n",
    "print (\"Test F1: \" + str(f1_score(flatten(z_test_slot),flatten(z_pred_slot_test), average = 'weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 63.4679715302%\n",
      "Test Accuracy: 60.02837189%\n"
     ]
    }
   ],
   "source": [
    "print (\"Train Accuracy: \" + str(accu(flatten(z_train_slot),flatten(z_pred_slot_train))*100) + \"%\")\n",
    "print (\"Test Accuracy: \" + str(accu(flatten(z_test_slot),flatten(z_pred_slot_test))*100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM+CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_embedding_layer = Embedding(len(embedding_matrix_w), len(embedding_matrix_w[0]), weights=[embedding_matrix_w], input_length=max_len, \n",
    "                            trainable=False)\n",
    "\n",
    "sequence_input = Input(shape=(max_len,), dtype='int32')\n",
    "\n",
    "lstm_embedded_sequences = lstm_embedding_layer(sequence_input)\n",
    "\n",
    "lstm = LSTM(200,return_sequences=True)(lstm_embedded_sequences)\n",
    "conv0=Conv1D(filters=100, kernel_size=3, activation='relu', padding='same',kernel_initializer='he_normal')(lstm)\n",
    "pool0 = MaxPooling1D(2)(conv0)\n",
    "pool0=Reshape((23*2,50))(pool0)\n",
    "out = Dense(units=200, activation='relu', kernel_initializer='he_normal')(pool0)\n",
    "\n",
    "out = Dense(units=len(slot_dict), activation='softmax', kernel_initializer='he_normal')(out)\n",
    "\n",
    "model5 = Model(inputs=sequence_input, outputs=out)\n",
    "model5.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 46)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 46, 300)           230100    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 46, 200)           400800    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 46, 100)           60100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 23, 100)           0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 46, 50)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 46, 200)           10200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 46, 127)           25527     \n",
      "=================================================================\n",
      "Total params: 726,727\n",
      "Trainable params: 496,627\n",
      "Non-trainable params: 230,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveBestModel15 = ModelCheckpoint(filepath=\"/home/sinchan/Videos/slot_filling/lstmcnn5.hdf5\", monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4978 samples, validate on 893 samples\n",
      "Epoch 1/10\n",
      " - 48s - loss: 1.1703 - acc: 0.6778 - categorical_accuracy: 0.6778 - val_loss: 1.0468 - val_acc: 0.9108 - val_categorical_accuracy: 0.9108\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91083, saving model to /home/sinchan/Videos/slot_filling/lstmcnn5.hdf5\n",
      "Epoch 2/10\n",
      " - 48s - loss: 1.1327 - acc: 0.9103 - categorical_accuracy: 0.9103 - val_loss: 1.0136 - val_acc: 0.9108 - val_categorical_accuracy: 0.9108\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.91083\n",
      "Epoch 3/10\n",
      " - 40s - loss: 1.0959 - acc: 0.9103 - categorical_accuracy: 0.9103 - val_loss: 0.9812 - val_acc: 0.9108 - val_categorical_accuracy: 0.9108\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.91083\n",
      "Epoch 4/10\n",
      " - 36s - loss: 1.0599 - acc: 0.9103 - categorical_accuracy: 0.9103 - val_loss: 0.9496 - val_acc: 0.9108 - val_categorical_accuracy: 0.9108\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.91083\n",
      "Epoch 5/10\n",
      " - 39s - loss: 1.0248 - acc: 0.9103 - categorical_accuracy: 0.9103 - val_loss: 0.9189 - val_acc: 0.9108 - val_categorical_accuracy: 0.9108\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.91083\n",
      "Epoch 6/10\n",
      " - 37s - loss: 0.9905 - acc: 0.9103 - categorical_accuracy: 0.9103 - val_loss: 0.8891 - val_acc: 0.9108 - val_categorical_accuracy: 0.9108\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.91083\n",
      "Epoch 7/10\n",
      " - 37s - loss: 0.9570 - acc: 0.9103 - categorical_accuracy: 0.9103 - val_loss: 0.8602 - val_acc: 0.9108 - val_categorical_accuracy: 0.9108\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.91083\n",
      "Epoch 8/10\n",
      " - 36s - loss: 0.9245 - acc: 0.9103 - categorical_accuracy: 0.9103 - val_loss: 0.8321 - val_acc: 0.9108 - val_categorical_accuracy: 0.9108\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.91083\n",
      "Epoch 9/10\n",
      " - 42s - loss: 0.8930 - acc: 0.9103 - categorical_accuracy: 0.9103 - val_loss: 0.8050 - val_acc: 0.9108 - val_categorical_accuracy: 0.9108\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.91083\n",
      "Epoch 10/10\n",
      " - 36s - loss: 0.8624 - acc: 0.9103 - categorical_accuracy: 0.9103 - val_loss: 0.7789 - val_acc: 0.9108 - val_categorical_accuracy: 0.9108\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.91083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f05ddfc9710>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.fit(np.array(train_sequence), np.array(y_train_slot),validation_data=(np.array(test_sequence),np.array(y_test_slot)), epochs=10, batch_size=50, verbose=2,callbacks=[saveBestModel15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred_train = model5.predict(train_sequence)\n",
    "lstm_pred_test =  model5.predict(test_sequence)\n",
    "lstm_pred_classes_train = lstm_pred_train.argmax(axis=-1)\n",
    "lstm_pred_classes_test = lstm_pred_test.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_pred_slot_train = make_z_pred_slot(lstm_pred_classes_train,z_train_slot)\n",
    "z_pred_slot_test = make_z_pred_slot(lstm_pred_classes_test,z_test_slot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1: 0.49284069196626834\n",
      "Test F1: 0.4503458217073495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sinchan/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print (\"Train F1: \" + str(f1_score(flatten(z_train_slot),flatten(z_pred_slot_train), average = 'weighted')))\n",
    "print (\"Test F1: \" + str(f1_score(flatten(z_test_slot),flatten(z_pred_slot_test), average = 'weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 63.4679715302%\n",
      "Test Accuracy: 60.02837189%\n"
     ]
    }
   ],
   "source": [
    "print (\"Train Accuracy: \" + str(accu(flatten(z_train_slot),flatten(z_pred_slot_train))*100) + \"%\")\n",
    "print (\"Test Accuracy: \" + str(accu(flatten(z_test_slot),flatten(z_pred_slot_test))*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-FOLD CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_kfold(k):\n",
    "    \n",
    "    train = pd.read_json('../input/train.json')\n",
    "    train.inc_angle = train.inc_angle.replace('na', 0)\n",
    "    \n",
    "    x_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "    x_band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "    x_band3 = x_band1 / x_band2\n",
    "       \n",
    "    X_train = np.concatenate([x_band1[:, :, :, np.newaxis]\n",
    "                            , x_band2[:, :, :, np.newaxis]\n",
    "                            , x_band3[:, :, :, np.newaxis]], axis=-1)\n",
    "                         \n",
    "    y_train = np.array(train[\"is_iceberg\"])\n",
    "    \n",
    "    folds = list(StratifiedKFold(n_splits=k, shuffle=True, random_state=1).split(X_train, y_train))\n",
    "    \n",
    "    return folds, X_train, y_train\n",
    "\n",
    "k = 7\n",
    "folds, X_train, y_train = load_data_kfold(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf=KFold(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine=raw_train+raw_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine=np.array(combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combine_slot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-1744582a0b67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcombine_slot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombine_slot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'combine_slot' is not defined"
     ]
    }
   ],
   "source": [
    "combine_slot=np.array(combine_slot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=kf.split(combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "766"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-0bd8b4daea4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'ed'"
     ]
    }
   ],
   "source": [
    "word_dict['ed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD=     1\n",
      "(4696, 46, 127)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 46, 300)           230100    \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 46, 32)            28832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 23, 32)            0         \n",
      "_________________________________________________________________\n",
      "reshape_8 (Reshape)          (None, 46, 16)            0         \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 46, 100)           46800     \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 46, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 46, 300)           30300     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 46, 127)           38227     \n",
      "=================================================================\n",
      "Total params: 454,659\n",
      "Trainable params: 224,559\n",
      "Non-trainable params: 230,100\n",
      "_________________________________________________________________\n",
      "[1175 1176 1177 ... 5868 5869 5870] [   0    1    2 ... 1172 1173 1174]\n",
      "Train on 4696 samples, validate on 1175 samples\n",
      "Epoch 1/10\n",
      " - 26s - loss: 0.6508 - acc: 0.8997 - categorical_accuracy: 0.8997 - val_loss: 0.4920 - val_acc: 0.9099 - val_categorical_accuracy: 0.9099\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.90988, saving model to /home/sinchan/Videos/slot_filling/lstmcnnkfold.hdf5\n",
      "Epoch 2/10\n",
      " - 22s - loss: 0.4323 - acc: 0.9105 - categorical_accuracy: 0.9105 - val_loss: 0.3928 - val_acc: 0.9099 - val_categorical_accuracy: 0.9099\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.90988\n",
      "Epoch 3/10\n",
      " - 22s - loss: 0.3127 - acc: 0.9099 - categorical_accuracy: 0.9099 - val_loss: 0.2644 - val_acc: 0.9091 - val_categorical_accuracy: 0.9091\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.90988\n",
      "Epoch 4/10\n",
      " - 22s - loss: 0.2359 - acc: 0.6991 - categorical_accuracy: 0.6991 - val_loss: 0.1903 - val_acc: 0.2050 - val_categorical_accuracy: 0.2050\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.90988\n",
      "Epoch 5/10\n",
      " - 22s - loss: 0.1672 - acc: 0.4756 - categorical_accuracy: 0.4756 - val_loss: 0.1434 - val_acc: 0.9320 - val_categorical_accuracy: 0.9320\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.90988 to 0.93199, saving model to /home/sinchan/Videos/slot_filling/lstmcnnkfold.hdf5\n",
      "Epoch 6/10\n",
      " - 22s - loss: 0.1340 - acc: 0.9138 - categorical_accuracy: 0.9138 - val_loss: 0.1187 - val_acc: 0.9373 - val_categorical_accuracy: 0.9373\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.93199 to 0.93730, saving model to /home/sinchan/Videos/slot_filling/lstmcnnkfold.hdf5\n",
      "Epoch 7/10\n",
      " - 22s - loss: 0.1148 - acc: 0.9136 - categorical_accuracy: 0.9136 - val_loss: 0.1036 - val_acc: 0.9404 - val_categorical_accuracy: 0.9404\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.93730 to 0.94044, saving model to /home/sinchan/Videos/slot_filling/lstmcnnkfold.hdf5\n",
      "Epoch 8/10\n",
      " - 23s - loss: 0.1016 - acc: 0.8988 - categorical_accuracy: 0.8988 - val_loss: 0.0914 - val_acc: 0.9352 - val_categorical_accuracy: 0.9352\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.94044\n",
      "Epoch 9/10\n",
      " - 22s - loss: 0.0897 - acc: 0.8361 - categorical_accuracy: 0.8361 - val_loss: 0.0812 - val_acc: 0.8524 - val_categorical_accuracy: 0.8524\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.94044\n",
      "Epoch 10/10\n",
      " - 22s - loss: 0.0803 - acc: 0.4477 - categorical_accuracy: 0.4477 - val_loss: 0.0728 - val_acc: 0.4365 - val_categorical_accuracy: 0.4365\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.94044\n",
      "Train F1: 0.912157317586842\n",
      "Test F1: 0.9021272942684511\n",
      "Train Accuracy: 92.9145907473%\n",
      "Test Accuracy: 92.2959406373%\n",
      "FOLD=     2\n",
      "(4697, 46, 127)\n",
      "[   0    1    2 ... 5868 5869 5870] [1175 1176 1177 ... 2346 2347 2348]\n",
      "Train on 4697 samples, validate on 1174 samples\n",
      "Epoch 1/10\n",
      " - 28s - loss: 0.6586 - acc: 0.8967 - categorical_accuracy: 0.8967 - val_loss: 0.5115 - val_acc: 0.9076 - val_categorical_accuracy: 0.9076\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.90762, saving model to /home/sinchan/Videos/slot_filling/lstmcnnkfold.hdf5\n",
      "Epoch 2/10\n",
      " - 25s - loss: 0.3874 - acc: 0.9111 - categorical_accuracy: 0.9111 - val_loss: 0.3251 - val_acc: 0.9076 - val_categorical_accuracy: 0.9076\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.90762\n",
      "Epoch 3/10\n"
     ]
    }
   ],
   "source": [
    "fold=0\n",
    "for train_index, test_index in kf.split(combine):\n",
    "    fold=fold+1\n",
    "    print \"FOLD=     \"+str(fold)\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    raw_train=combine[train_index]\n",
    "    raw_test=combine[test_index]\n",
    "    train = []\n",
    "    replace_dic = {'\\'d' : \"would\" , '\\'s' : \"is\" , '\\'t' : \"not\"}\n",
    "    for sentence in raw_train:\n",
    "        sentence = sentence.split('\\t')[0][4:-4]\n",
    "        words = sentence.split(' ')\n",
    "        new_sentence = \"\"\n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "            word = word.split(\"'\")[0]\n",
    "            if(word == ''):\n",
    "                print word\n",
    "                continue;\n",
    "            if(word in replace_dic):\n",
    "                print(word)\n",
    "                word = replace_dic[word];\n",
    "            if(word.isdigit()):\n",
    "                word = \"number\"\n",
    "            new_sentence = new_sentence + \" \" + word\n",
    "        train.append(new_sentence[1:])\n",
    "    test = []\n",
    "    replace_dic = {'\\'d' : \"would\" , '\\'s' : \"is\" , '\\'t' : \"not\"}\n",
    "    for sentence in raw_test:\n",
    "        sentence = sentence.split('\\t')[0][4:-4]\n",
    "        words = sentence.split(' ')\n",
    "        new_sentence = \"\"\n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "            word = word.split(\"'\")[0]\n",
    "            if(word == ''):\n",
    "                continue;\n",
    "            if(word in replace_dic):\n",
    "                word = replace_dic[word];\n",
    "            if(word.isdigit()):\n",
    "                word = \"number\"\n",
    "            new_sentence = new_sentence + \" \" + word\n",
    "        test.append(new_sentence[1:])\n",
    "    \n",
    "    x_train = make_sequences(train,word_dict)\n",
    "    x_test = make_sequences(test,word_dict)\n",
    "    x_train = pad_sequences(x_train, maxlen = 46, padding='post')\n",
    "    x_test = pad_sequences(x_test, maxlen = 46, padding='post')\n",
    "    n = len(slot_dict)\n",
    "\n",
    "    n = len(slot_dict)\n",
    "\n",
    "    y_train_slot = list()\n",
    "    for utterance in raw_train:\n",
    "        outs = np.zeros((max_len,len(slot_dict)))\n",
    "        outs = list(outs)\n",
    "        slots = utterance.split('\\t')[1].strip().split(' ')[1:-1]\n",
    "        for i in range(len(slots)):\n",
    "            outs[i][slot_dict[slots[i]]] = 1\n",
    "        y_train_slot.append(outs)\n",
    "    \n",
    "    y_test_slot = list()\n",
    "    for utterance in raw_test:\n",
    "        outs = np.zeros((max_len,len(slot_dict)))\n",
    "        outs = list(outs)\n",
    "        slots = utterance.split('\\t')[1].strip().split(' ')[1:-1]\n",
    "        for i in range(len(slots)):\n",
    "            outs[i][slot_dict[slots[i]]] = 1\n",
    "        y_test_slot.append(outs)\n",
    "    print np.shape(y_train_slot)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(embedding_matrix_g), len(embedding_matrix_g[0]), weights=[embedding_matrix_g], input_length=46, trainable=False))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Reshape((23*2,16)))\n",
    "    model.add(LSTM(100, dropout=0.2, return_sequences=True))\n",
    "    model.add(LSTM(100, dropout=0.2, return_sequences=True))\n",
    "    model.add(Dense(300,activation='relu',kernel_initializer='he_normal'))\n",
    "    model.add(Dense(len(slot_dict), activation=\"sigmoid\",kernel_initializer='he_normal',input_shape=(46,)))\n",
    "    if(fold==1):\n",
    "        model.summary()\n",
    "    print train_index,test_index\n",
    "    model.save('rnncnn')\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','categorical_accuracy'])\n",
    "    saveBestModel15 = ModelCheckpoint(filepath=\"/home/sinchan/Videos/slot_filling/lstmcnnkfold.hdf5\", monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "    model.fit(np.array(x_train),np.array(y_train_slot),validation_data=(np.array(x_test),np.array(y_test_slot)), epochs=10, batch_size=50, verbose=2,callbacks=[saveBestModel15])\n",
    "    lstm_pred_train = model.predict(train_sequence)\n",
    "    lstm_pred_test =  model.predict(test_sequence)\n",
    "    lstm_pred_classes_train = lstm_pred_train.argmax(axis=-1)\n",
    "    lstm_pred_classes_test = lstm_pred_test.argmax(axis=-1)\n",
    "    z_pred_slot_train = make_z_pred_slot(lstm_pred_classes_train,z_train_slot)\n",
    "    z_pred_slot_test = make_z_pred_slot(lstm_pred_classes_test,z_test_slot)\n",
    "    print (\"Train F1: \" + str(f1_score(flatten(z_train_slot),flatten(z_pred_slot_train), average = 'weighted')))\n",
    "    print (\"Test F1: \" + str(f1_score(flatten(z_test_slot),flatten(z_pred_slot_test), average = 'weighted')))\n",
    "    print (\"Train Accuracy: \" + str(accu(flatten(z_train_slot),flatten(z_pred_slot_train))*100) + \"%\")\n",
    "    print (\"Test Accuracy: \" + str(accu(flatten(z_test_slot),flatten(z_pred_slot_test))*100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1174, 46)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4978,46) (893,46) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-81491148e1f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcombine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_sequence\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtest_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4978,46) (893,46) "
     ]
    }
   ],
   "source": [
    "combine=train_sequence+test_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
